# tasks.yaml
research_task:
  description: >
    Conduct a comprehensive search for papers on: {topic}.
    
    MANDATORY EXECUTION PLAN:
    1. ANALYZE the topic to identify core entities (Models, Methods, Domains).
    2. REFINE the query by removing stop words (e.g., remove "using", "application of", "based on").
       - Example Input: "Predicting mortality rate using AI"
       - Refined Search: "Predicting mortality rate AI"
    3. CALL "OpenAlex Search" with this optimized query string.
    4. If results are poor, try a broader variation (e.g., remove specific model names).
    5. Compile the valid results found.
    
    DO NOT proceed without calling the tool.
  
  expected_output: >
    A list of REAL papers fetched from OpenAlex. Each entry must have:
    - Title
    - Year
    - Author
    - Link (DOI/URL)
    - Abstract
    - Source ("OpenAlex" or "Memory")
  
  agent: librarian

review_task:
  description: >
    You are the Critic. Your job is to review every paper the Librarian provided and produce a clear, structured approval list for the Scribe.
    
    The search query used for this research was: "{topic}". Use it to judge relevance.
    
    MANDATORY STEPS (do them in order):
    
    1. LIST EACH PAPER
       - Go through the Librarian's output one by one. For each paper, note: Title, Year, Author, Link, and Abstract (when provided).
    
    2. VALIDATE EACH PAPER (Abstract is PRIMARY over Title)
       For each paper, check:
       - LINK: Does it have a valid Link (DOI or OpenAlex URL)? If missing or invalid â†’ REJECT.
       - SOURCE: Is the paper in the Librarian's list (not made up)? If not in the list â†’ REJECT.
       - RELEVANCE (Abstract has priority over Title):
         * When Abstract is provided (not "(not provided)"): The abstract is the PRIMARY source. If the abstract clearly relates to the query (same domain, key terms, research question) â†’ ACCEPT. If the abstract does NOT relate to the query â†’ REJECT. Do NOT reject based on title alone when the abstract supports relevance.
         * When Abstract is "(not provided)" (e.g. from Memory): Use Title only as fallback; be slightly more lenient but reject clearly off-topic papers.
    
    3. DECIDE
       - ACCEPT: Valid link, in Librarian's list, and (when abstract present) abstract supports relevance. Abstract outweighs title.
       - REJECT: Missing link, not in list, or (when abstract present) abstract does not support relevance.
    
    4. OUTPUT FORMAT
       - First, a short summary: "Reviewed N papers. Approved M; rejected K."
       - Then a numbered list of APPROVED papers only, each line: Title | Author | Year | Link
       - Do NOT include rejected papers in the final list.
    
    IDENTIFY the "Winner" (Highest Quality Score).IDENTIFY the "Contenders" (The rest).
    DO NOT approve any paper without a link. DO NOT add papers that were not in the Librarian's output. When abstract is present, use it to check query relevance.
  expected_output: >
    A short review summary (counts of reviewed / approved / rejected) followed by a numbered list of APPROVED papers only, each with Title, Author, Year, Link. Relevance must be verified using the Abstract (from abstract_inverted_index when from API) against the search query; no fake or unlinked papers. A structured list separating the "Winner" (Best Paper) from the "Contenders.
  agent: critic

synthesis_task:
  description: >
    Write the final report as Dr. Kimi. ðŸ‘©â€ðŸ”¬
    Follow this EXACT structure:
    
    # âš¡ Research Brief: [Topic]
    
    ### ðŸ§ The TL;DR (Executive Summary)
    - (The main takeaway) - How it relates to users query asked by user in the input.
    - (The state of the art) - How it relates to users query asked by user in the input.
    
    ### ðŸ† The Leaderboard (Rapid-Fire Comparison)
    | Paper Title | The "Secret Sauce" (Method) | The "Catch" (Limitations) | Verdict |
    | :--- | :--- | :--- | :--- |
    | [Paper A] | [e.g. Quantized LLaMA] | [e.g. High VRAM] | ðŸ¥‡ GOAT |
    | [Paper B] | ... | ... | ðŸ¥ˆ Solid |
    
    ### ðŸ’Ž The Deep Dive: [Title of Best Paper]
    - **Why it wins:** (Technical breakdown)
    - **The Tech Stack:** (Model architecture, dataset used)
    - **â˜ï¸ðŸ˜ºDr. Kimi's Take:** (Is it actually useful? Or just academic fluff?). How it relates to users query asked by user in the input.
    
    ### ðŸŒ¶ï¸ The Spicy Critical Take
    - (Roast the current state of the field. What is everyone doing wrong? What is "mid"?)
    
    ### ðŸ”® Future Outlook (2026)
    - (Where is this tech going next?)
    
    ### ðŸ“š Verified References
    | Title | Author | Year | Why it's Relevant | Link |
    | :--- | :--- | :--- | :--- | :--- |
    | [Title] | [Name] | [Year] | [1 sentence justification to query asked by user] | [DOI] |
  expected_output: >
    A Markdown report with emojis, a comparison table, a deep dive section, and a final reference table.
  agent: scribe