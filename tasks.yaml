# tasks.yaml
research_task:
  description: >
    Conduct a comprehensive search for papers on: {topic}.
    
    MANDATORY EXECUTION PLAN:
    1. ANALYZE the topic to identify core entities (Models, Methods, Domains).
    2. REFINE the query by removing stop words (e.g., remove "using", "application of", "based on").
       - Example Input: "Predicting mortality rate using AI"
       - Refined Search: "Predicting mortality rate AI"
    3. CALL "OpenAlex Search" with this optimized query string.
    4. If results are poor, try a broader variation (e.g., remove specific model names).
    5. Compile the valid results found.
    
    DO NOT proceed without calling the tool.
  
  expected_output: >
    A list of REAL papers fetched from OpenAlex. Each entry must have:
    - Title
    - Year
    - Author
    - Link (DOI/URL)
    - Abstract
    - Source ("OpenAlex" or "Memory")
  
  agent: librarian

review_task:
  description: >
    You are the Critic. Your job is to review every paper the Librarian provided and produce a clear, structured approval list for the Scribe.
    
    The search query used for this research was: "{topic}". Use it to judge relevance.
    
    MANDATORY STEPS (do them in order):
    
    1. LIST EACH PAPER
       - Go through the Librarian's output one by one. For each paper, note: Title, Year, Author, Link, and Abstract (when provided).
    
    2. VALIDATE EACH PAPER (Abstract is PRIMARY over Title)
       For each paper, check:
       - LINK: Does it have a valid Link (DOI or OpenAlex URL)? If missing or invalid → REJECT.
       - SOURCE: Is the paper in the Librarian's list (not made up)? If not in the list → REJECT.
       - RELEVANCE (Abstract has priority over Title):
         * When Abstract is provided (not "(not provided)"): The abstract is the PRIMARY source. If the abstract clearly relates to the query (same domain, key terms, research question) → ACCEPT. If the abstract does NOT relate to the query → REJECT. Do NOT reject based on title alone when the abstract supports relevance.
         * When Abstract is "(not provided)" (e.g. from Memory): Use Title only as fallback; be slightly more lenient but reject clearly off-topic papers.
    
    3. DECIDE
       - ACCEPT: Valid link, in Librarian's list, and (when abstract present) abstract supports relevance. Abstract outweighs title.
       - REJECT: Missing link, not in list, or (when abstract present) abstract does not support relevance.
    
    4. OUTPUT FORMAT
       - First, a short summary: "Reviewed N papers. Approved M; rejected K."
       - Then a numbered list of APPROVED papers only, each line: Title | Author | Year | Link
       - Do NOT include rejected papers in the final list.
    
    DO NOT approve any paper without a link. DO NOT add papers that were not in the Librarian's output. When abstract is present, use it to check query relevance.
  expected_output: >
    A short review summary (counts of reviewed / approved / rejected) followed by a numbered list of APPROVED papers only, each with Title, Author, Year, Link. Relevance must be verified using the Abstract (from abstract_inverted_index when from API) against the search query; no fake or unlinked papers.
  agent: critic

synthesis_task:
  description: >
    Take the filtered list of papers and write a 3-section research report.
    You MUST include a 'References' section at the bottom.
  expected_output: >
    A professional Markdown report with:
    1. Executive Summary briefly summarizing the report and the key findings.
    2. suggest direct answer to the topic in bullets points briefly.
    3. References (List of original papers titles, authors, and publication years with their clickable Links) tabular format.
    4. Comparison of the papers with the topic, and how much they are related to the topic in tabular format.
    If data is missing, leave the row blank or say 'N/A'.
  agent: scribe